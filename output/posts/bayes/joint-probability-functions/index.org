#+BEGIN_COMMENT
.. title: Joint Probability Functions
.. slug: joint-probability-functions
.. date: 2019-06-12 12:33:59 UTC-07:00
.. tags: bayes,probability
.. category: Bayes' Rule
.. link: 
.. description: Looking at Joint Probability Functions.
.. type: text
.. status: 
.. updated: 

#+END_COMMENT
#+OPTIONS: ^:{}
#+OPTIONS: H:5
#+TOC: headlines 2
#+BEGIN_SRC ipython :session joint :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  This looks at the case where we have more than one input and more than one possible output value. This assumes the possible outputs are discrete. 
** Imports
*** Python
#+begin_src ipython :session joint :results none
from functools import partial
from pathlib import Path
#+end_src
*** PyPi
#+begin_src ipython :session joint :results none
import holoviews
import pandas
#+end_src
*** My Stuff
#+begin_src ipython :session joint :results none
from graeae.visualization import EmbedHoloview
#+end_src
** The Embed
#+begin_src ipython :session joint :results none
SLUG = "joint-probability-functions"
output = Path("../../files/posts/bayes/")/SLUG
Embed = partial(EmbedHoloview, folder_path = output)
#+end_src
* Middle
** The Joint Probability Function
The *Joint Probability Function* is the distribution of input and output probabilities for each joint probability. To find the probabilities we need to take measurement counts. If we had two types of data and three hypotheses we could write out our counts in a table like this.

| Data  | $\theta_1$ | $\theta_2$ | $\theta_3$ | Sum |
|-------+------------+------------+------------+-----|
| $x_1$ |          2 |          0 |          1 |   3 |
| $x_2$ |          1 |          4 |          2 |   7 |
|-------+------------+------------+------------+-----|
| Sum   |          3 |          4 |          3 |  10 |

The intersection of data and hypothesis is the count we got for our joint probability calculation. To get the actual joint probability we divide the numbers by the total count.

| Data  | $\theta_1$ | $\theta_2$ | $\theta_3$ | Sum |
|-------+------------+------------+------------+-----|
| $x_1$ |        0.2 |          0 |        0.1 | 0.3 |
| $x_2$ |        0.1 |        0.4 |        0.2 | 0.7 |
|-------+------------+------------+------------+-----|
| Sum   |        0.3 |        0.4 |        0.3 |   1 |

So, for instance, the joint probability $p(x_2, \theta_2)$ is 0.1. The summary values at the bottom are the /marginal likelihood of the hypotheses/ - $p(\theta_2) = 0.4$ - and the summary values on the far right are the /marginal likelihood of the data/ - $p(x_2) = 0.7$.

** Visualizing It
#+begin_src ipython :session joint :results none
data = pandas.DataFrame([
    [2, 0, 1],
    [1, 4, 2],
], columns=["theta_1", "theta_2", "theta_3"], index=["x_1", "x_2"])
#+end_src

#+begin_src ipython :session joint :results output raw :exports both

#+end_src
* End
** Source
1. Stone JV. Bayesâ€™ rule: a tutorial introduction to Bayesian analysis. First edition, third printing [with corrections]. Sheffield: Sebtel Press; 2014. 170 p. 
