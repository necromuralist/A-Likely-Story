<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A Likely Story (Posts about probability)</title><link>https://necromuralist.github.io/A-Likely-Story/</link><description></description><atom:link href="https://necromuralist.github.io/A-Likely-Story/categories/probability.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Mon, 17 Jun 2019 23:49:41 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Joint Probability Functions</title><link>https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#org1ef57da"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#orgc7e6e10"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#orge94f470"&gt;The Plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#orge7ce802"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#orga17118f"&gt;The Joint Probability Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#org49eeb2f"&gt;Disesases And Symptoms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#org2923dc4"&gt;The Probability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#org895a5e6"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#org1350896"&gt;Tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/#org45dd9eb"&gt;Source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1ef57da" class="outline-2"&gt;
&lt;h2 id="org1ef57da"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1ef57da"&gt;
&lt;p&gt;
This looks at the case where we have more than one input and more than one possible output value. This assumes the possible outputs are discrete. 
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc7e6e10" class="outline-3"&gt;
&lt;h3 id="orgc7e6e10"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc7e6e10"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5a77900" class="outline-4"&gt;
&lt;h4 id="org5a77900"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5a77900"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from argparse import Namespace
from functools import partial
from math import isclose
from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf894030" class="outline-4"&gt;
&lt;h4 id="orgf894030"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf894030"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from bokeh.models import HoverTool
from expects import (
    be,
    be_true,
    equal,
    expect,
)
from tabulate import tabulate
import holoviews
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfdce8ff" class="outline-4"&gt;
&lt;h4 id="orgfdce8ff"&gt;My Stuff&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfdce8ff"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae.visualization import EmbedHoloview
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge94f470" class="outline-3"&gt;
&lt;h3 id="orge94f470"&gt;The Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge94f470"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org53b2f78" class="outline-4"&gt;
&lt;h4 id="org53b2f78"&gt;The Holoviews Embedder&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org53b2f78"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "joint-probability-functions"
output = Path("../../files/posts/bayes/")/SLUG
holoviews.extension("bokeh")
Embed = partial(EmbedHoloview, folder_path = output)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9e488a9" class="outline-4"&gt;
&lt;h4 id="org9e488a9"&gt;The Plot Settings&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9e488a9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Plot = Namespace(
    width=800,
    height=800,
)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge7ce802" class="outline-2"&gt;
&lt;h2 id="orge7ce802"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge7ce802"&gt;
&lt;p&gt;
Although Bayes Rule is often shown as being a binary hypothesis (you have a disease or you don't) with one data input (you tested positive for the disease), in most cases you will have more than one of each. Creating a table of counts/probabilities will make it easier to work with these hypotheses and data.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga17118f" class="outline-3"&gt;
&lt;h3 id="orga17118f"&gt;The Joint Probability Function&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga17118f"&gt;
&lt;p&gt;
The &lt;b&gt;Joint Probability Function&lt;/b&gt; is the distribution of input and output probabilities for each joint probability. To find the probabilities we need to take measurement counts. If we had two types of data and three hypotheses we could write out our counts in a table like this.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Data&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;\(\theta_1\)&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;\(\theta_2\)&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;\(\theta_3\)&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Sum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;\(x_1\)&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;\(x_2\)&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-right"&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Sum&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
The intersection of data and hypothesis is the count we got for our joint probability calculation. To get the actual joint probability we divide the numbers by the total count.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Data&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;\(\theta_1\)&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;\(\theta_2\)&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;\(\theta_3\)&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Sum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;\(x_1\)&lt;/td&gt;
&lt;td class="org-right"&gt;0.2&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0.1&lt;/td&gt;
&lt;td class="org-right"&gt;0.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;\(x_2\)&lt;/td&gt;
&lt;td class="org-right"&gt;0.1&lt;/td&gt;
&lt;td class="org-right"&gt;0.4&lt;/td&gt;
&lt;td class="org-right"&gt;0.2&lt;/td&gt;
&lt;td class="org-right"&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Sum&lt;/td&gt;
&lt;td class="org-right"&gt;0.3&lt;/td&gt;
&lt;td class="org-right"&gt;0.4&lt;/td&gt;
&lt;td class="org-right"&gt;0.3&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
So, for instance, the joint probability \(p(x_2, \theta_2)\) is 0.1. The summary values at the bottom are the &lt;i&gt;marginal likelihood of the hypotheses&lt;/i&gt; - \(p(\theta_2) = 0.4\) - and the summary values on the far right are the &lt;i&gt;marginal likelihood of the data&lt;/i&gt; - \(p(x_2) = 0.7\).
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb53b9e0" class="outline-4"&gt;
&lt;h4 id="orgb53b9e0"&gt;Visualizing It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb53b9e0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame([
    [2, 1],
    [0, 4],
    [1, 2]
], columns=["x_1", "x_2"])
data["theta_likelihood"] = data.x_1 + data.x_2
data = data.reset_index().rename(columns={"index": "theta"})
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;height = Plot.height - 300
scatter_1 = holoviews.Scatter(data, vdims=["x_1"], kdims=["theta"]).opts(
    jitter=0.1,
    size=holoviews.dim("x_1")*10 + 5,
).opts(
    padding=0.5,
    height=height,
    width=Plot.width,
    tools=["hover"],
    xticks=3,
    yticks=3,
)

scatter_2 = holoviews.Scatter(data, vdims=["x_2"], kdims=["theta"]).opts(
    jitter=0.1,
    size=holoviews.dim("x_2")*10 + 5,
).opts(
    padding=0.5,
    height=height,
    width=Plot.width,
    tools=["hover"],
    xticks=3,
    yticks=3,
)

scattered = (scatter_1 * scatter_2).opts(ylabel="X")
spikes = holoviews.Spikes(data, "theta", "theta_likelihood").opts(
    width=Plot.width,
    xaxis=None,
    yticks=4,
    height=200,
)

x_likelihood = data[["x_1", "x_2"]].sum().reset_index().rename(columns={0: "X Likelihood"})

spikes_2 = holoviews.Spikes(x_likelihood, "index", "X Likelihood").opts(
height=height)
plot = scattered &amp;lt;&amp;lt; spikes_2 &amp;lt;&amp;lt; spikes
# plot = ((spikes + scattered ).cols(1) + spikes_2).opts(title="Theta vs X")
Embed(plot=plot, file_name="theta_vs_x1_toy")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/theta_vs_x1_toy.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org49eeb2f" class="outline-3"&gt;
&lt;h3 id="org49eeb2f"&gt;Disesases And Symptoms&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org49eeb2f"&gt;
&lt;p&gt;
Suppose you have ten diseases and four symptoms and you count the number of patients that have a disease and a symptom. You can show the Joint Probability as a table.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;joint = pandas.DataFrame(
    [
	[0,0,1,0,3,5,10,7,7,4],
	[0,1,1,10,16,11,12,7,8,5],
	[3,5,8,9,14,10,3,3,0,0],
	[8,9,9,5,4,1,1,0,0,0],
    ],
    columns=["theta0", "theta1", "theta2", "theta3", "theta4", "theta5", 
	     "theta6", "theta7", "theta8", "theta9"],
    index=["x0", "x1", "x2", "x3"],
)

assert joint.shape == (4, 10)
assert all(joint.T.sum() == pandas.Series([37, 71, 55, 37], 
["x0", "x1", "x2", "x3"])), joint.T.sum()
assert joint.T.sum().sum() == 200
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org886483b" class="outline-4"&gt;
&lt;h4 id="org886483b"&gt;Finding a Joint Probability&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org886483b"&gt;
&lt;p&gt;
&lt;i&gt;What is the /joint probability&lt;/i&gt; that a patient has the symptom \(x_1\) and disease \(\theta_1\) (\(p(x_1, \theta_1)\))? To answer this we can look up the value in the row for \(x_1\) and the column for \(\theta_1\) and divide it by the total number of patients.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;total = joint.sum().sum()
probabilities = joint/total
print(tabulate(probabilities, headers="keys", tablefmt="orgtbl"))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt; &lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta0&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta1&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta2&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta3&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta4&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta5&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta6&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta7&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta8&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;theta9&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;x0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0.005&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0.015&lt;/td&gt;
&lt;td class="org-right"&gt;0.025&lt;/td&gt;
&lt;td class="org-right"&gt;0.05&lt;/td&gt;
&lt;td class="org-right"&gt;0.035&lt;/td&gt;
&lt;td class="org-right"&gt;0.035&lt;/td&gt;
&lt;td class="org-right"&gt;0.02&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;x1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0.005&lt;/td&gt;
&lt;td class="org-right"&gt;0.005&lt;/td&gt;
&lt;td class="org-right"&gt;0.05&lt;/td&gt;
&lt;td class="org-right"&gt;0.08&lt;/td&gt;
&lt;td class="org-right"&gt;0.055&lt;/td&gt;
&lt;td class="org-right"&gt;0.06&lt;/td&gt;
&lt;td class="org-right"&gt;0.035&lt;/td&gt;
&lt;td class="org-right"&gt;0.04&lt;/td&gt;
&lt;td class="org-right"&gt;0.025&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;x2&lt;/td&gt;
&lt;td class="org-right"&gt;0.015&lt;/td&gt;
&lt;td class="org-right"&gt;0.025&lt;/td&gt;
&lt;td class="org-right"&gt;0.04&lt;/td&gt;
&lt;td class="org-right"&gt;0.045&lt;/td&gt;
&lt;td class="org-right"&gt;0.07&lt;/td&gt;
&lt;td class="org-right"&gt;0.05&lt;/td&gt;
&lt;td class="org-right"&gt;0.015&lt;/td&gt;
&lt;td class="org-right"&gt;0.015&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;x3&lt;/td&gt;
&lt;td class="org-right"&gt;0.04&lt;/td&gt;
&lt;td class="org-right"&gt;0.045&lt;/td&gt;
&lt;td class="org-right"&gt;0.045&lt;/td&gt;
&lt;td class="org-right"&gt;0.025&lt;/td&gt;
&lt;td class="org-right"&gt;0.02&lt;/td&gt;
&lt;td class="org-right"&gt;0.005&lt;/td&gt;
&lt;td class="org-right"&gt;0.005&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;intermediate = {
    "p(x1, theta_1)": probabilities.loc["x1", "theta1"]
}
print(f"p(x1, theta1) = {intermediate['p(x1, theta_1)']}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
p(x1, theta1) = 0.005

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb43cd57" class="outline-4"&gt;
&lt;h4 id="orgb43cd57"&gt;What is the probability \(p(x_1)\) that a patient has symptoms \(x_1\)?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb43cd57"&gt;
&lt;p&gt;
You can calculate this by counting all the patients that had symptom \(x_1\) (the sum of the row in the table across all thetas) and dividing by the total sample size.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;intermediate["p(x1)"] = probabilities.loc["x1"].sum()
print(f"p(x1) = {intermediate['p(x1)']:.3f}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
p(x1) = 0.355

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd294c86" class="outline-4"&gt;
&lt;h4 id="orgd294c86"&gt;What is the probability \(p(\theta_1)\) that a patient has the disease \(\theta_1\)?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd294c86"&gt;
&lt;p&gt;
This calculation is similar to calculating the probability of symptom \(x_1\) except instead of using a row total you use the column for \(\theta_1\).
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;intermediate["p(theta_1)"] = probabilities.theta1.sum()
print(f"p(theta_1) = {intermediate['p(theta_1)']}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
p(theta_1) = 0.075

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0cfca33" class="outline-4"&gt;
&lt;h4 id="org0cfca33"&gt;What is the conditional probability that a patient has the symptom \(x_1\) given that he has the disesase \(\theta_1\)?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0cfca33"&gt;
&lt;p&gt;
This is the count of patients with symptom \(x_1\) in the \(\theta_1\) column divided by the total number in the \(\theta_1\) column.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;intermediate["p(x1|theta_1)"] = probabilities.loc["x1", "theta1"]/probabilities["theta1"].sum()
print(f"p(x1|theta_1) = {intermediate['p(x1|theta_1)']:.3f}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
p(x1|theta_1) = 0.067

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org49fe2b6" class="outline-4"&gt;
&lt;h4 id="org49fe2b6"&gt;What is the conditional probability that a patent with disease \(\theta_1\) has symptom \(x_1\) (\(p(\theta_1| x_1)\))?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org49fe2b6"&gt;
&lt;p&gt;
Here's where we get to apply Bayes' Rule.
\[
p(\theta_1 | x_1) = \frac{p(x_1|\theta_1) p(\theta_1)}{p(x_1)}
\]
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;intermediate["p(theta_1|x_1)"] = ((
    intermediate["p(x1|theta_1)"] * intermediate["p(theta_1)"])
				  /intermediate["p(x1)"])
print(f"p(theta_1|x_1) = {intermediate['p(theta_1|x_1)']:.3f}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
p(theta_1|x_1) = 0.014

&lt;/pre&gt;

&lt;p&gt;
But, in fact, you can calculate this a little more directly using:
&lt;/p&gt;

&lt;p&gt;
\[
p(\theta_1|x_1) = \frac{p(x_1, \theta_1)}{p(x_1)}
\]
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;alternative = intermediate["p(x1, theta_1)"]/intermediate["p(x1)"]
print(f"Original: {intermediate['p(theta_1|x_1)']:.3}, Alternative: {alternative:.3}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Original: 0.0141, Alternative: 0.0141

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2923dc4" class="outline-3"&gt;
&lt;h3 id="org2923dc4"&gt;The Probability&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2923dc4"&gt;
&lt;p&gt;
This is just something to generalize what I did above.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class JointProbability:
    """A joint probability queryior

    Args:
     counts: table of counts for joint probability
     hypothesis: column with the hypothesis counts
     data: row with the data counts
    """
    def __init__(self, counts: pandas.DataFrame, hypothesis: str, 
		 data: str) -&amp;gt; None:
	self.counts = counts
	self.hypothesis = hypothesis
	self.data = data
	self._sum_total = None
	self._joint_probability = None
	self._data_probability = None
	self._hypothesis_probability = None
	self._probability_of_data_given_hypothesis = None
	self._probability_of_hypothesis_given_data = None
	self._maximum_a_priori = None
	self._probabilities = None
	return

    @property
    def sum_total(self) -&amp;gt; int:
	"""The total count of entries in the table"""
	if self._sum_total is None:
	    self._sum_total = self.counts.sum().sum()
	return self._sum_total

    @property
    def probabilities(self) -&amp;gt; pandas.DataFrame:
	"""The counts converted to probabilities"""
	if self._probabilities is None:
	    self._probabilities = self.counts/self.sum_total
	return self._probabilities

    @property
    def joint_probability(self) -&amp;gt; float:
	"""the joint probability of the data and hypothesis"""
	if self._joint_probability is None:
	    self._joint_probability = self.probabilities.loc[self.data, self.hypothesis]
	return self._joint_probability

    @property
    def data_probability(self) -&amp;gt; float:
	"""The probability of the data"""
	if self._data_probability is None:
	    self._data_probability = self.probabilities.loc[self.data].sum()
	return self._data_probability

    @property
    def hypothesis_probability(self) -&amp;gt; float:
	"""The probability of the hypothesis"""
	if self._hypothesis_probability is None:
	    self._hypothesis_probability = self.probabilities[
		self.hypothesis].sum()
	return self._hypothesis_probability

    @property
    def probability_of_data_given_hypothesis(self) -&amp;gt; float:
	"""The probability of our data given the hypothesis"""
	if self._probability_of_data_given_hypothesis is None:
	    self._probability_of_data_given_hypothesis = (
		self.probabilities.loc[self.data, self.hypothesis]
		/self.probabilities[self.hypothesis].sum()
	    )
	return self._probability_of_data_given_hypothesis

    @property
    def probability_of_hypothesis_given_data(self) -&amp;gt; float:
	"""The probability of our hypothesis given our data"""
	if self._probability_of_hypothesis_given_data is None:
	    self._probability_of_hypothesis_given_data = (
		self.joint_probability/self.data_probability)
	return self._probability_of_hypothesis_given_data

    @property
    def maximum_a_priori(self) -&amp;gt; str:
	"""The name of the most likely hypothesis"""
	if self._maximum_a_priori is None:
	    self._maximum_a_priori = (
		self.probabilities.loc[self.data]
		/self.data_probability).idxmax()
	return self._maximum_a_priori
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3e83f45" class="outline-4"&gt;
&lt;h4 id="org3e83f45"&gt;Visualize the MAP&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3e83f45"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6f5e404" class="outline-5"&gt;
&lt;h5 id="org6f5e404"&gt;The Probabilities&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-org6f5e404"&gt;
&lt;p&gt;
I'll use the &lt;code&gt;JointProbability&lt;/code&gt; class, although in this case aren't looking at values for a specific theta.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;table = JointProbability(joint, "theta1", data="x1")
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;height = int((Plot.height - 300)/3)
likelihood = (table.counts.loc["x1"]/table.counts.sum()).reset_index().rename(columns={0: "x1"})
likelihood_spikes = holoviews.Spikes(likelihood, vdims=["x1"], kdims=["index"]).opts(
).opts(
    padding=0,
    height=height,
    width=Plot.width,
    tools=["hover"],
    xlabel="Likelihood (p(x|theta))"
)

prior = (table.counts.sum()/table.counts.sum().sum()).reset_index().rename(columns={0: "x1"})
prior_spikes = holoviews.Spikes(prior, vdims=["x1"], kdims=["index"]).opts(
    height = height,
    width = Plot.width,
)

posterior = ((likelihood["x1"] * prior["x1"])/table.data_probability).reset_index()

posterior_spikes = holoviews.Spikes(posterior, vdims=["x1"], kdims=["index"]).opts(
    height=height,
    width=Plot.width,
)

plot = (likelihood_spikes + prior_spikes + posterior_spikes).cols(1).opts(title="Disease Probabilities for x1")
Embed(plot=plot, file_name="disease_probabilities")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/disease_probabilities.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org895a5e6" class="outline-2"&gt;
&lt;h2 id="org895a5e6"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org895a5e6"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1350896" class="outline-3"&gt;
&lt;h3 id="org1350896"&gt;Tests&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1350896"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class TestProbability:
    def __init__(self, table: pandas.DataFrame, data: str="x1", hypothesis: str="theta1"):
	self.table = table
	self.data = data
	self.hypothesis = hypothesis
	self._p_test = None
	return

    @property
    def p_test(self) -&amp;gt; JointProbability:
	"""The thing under test"""
	if self._p_test is None:
	    self._p_test = JointProbability(self.table, 
					    hypothesis=self.hypothesis,
					    data=self.data)
	return self._p_test

    def test_construction(self):
	# given an instance of the probability
	# when the table is retrieved
	actual = self.p_test.counts
	# then it is the expected
	expect(actual).to(be(self.table))
	# and the total is the expected
	expect(self.p_test.sum_total).to(equal(200))
	return

    def test_joint_probability(self):
	# given an instance of the probability
	# when the p(x1, theta1) is retrieved
	actual = self.p_test.joint_probability
	# then it is the expected value
	expect(isclose(actual, 0.005)).to(be_true)
	return

    def test_data_probabilitiy(self):
	# Given an instance of the probability
	# When p(x1) is retrieved
	actual = self.p_test.data_probability
	# Then it is the expected value
	expect(isclose(actual, 0.355)).to(be_true)
	return

    def test_hypothesis_probability(self):
	# Given an instance of the probability
	# When p(theta1) is retrieved
	actual = self.p_test.hypothesis_probability
	# Then it is the expected value
	expect(isclose(actual, 0.075)).to(be_true)
	return

    def test_probability_of_data_given_hypothesis(self):
	# Given an instance of the probability
	# When p(x1 | theta1) is retrieved
	actual = self.p_test.probability_of_data_given_hypothesis
	# Then it is the expected value
	expect(isclose(actual, 0.067, abs_tol=1e-3)).to(be_true)
	return

    def test_probability_of_hypothesis_given_data(self):
	# Given an instance of the probability
	# When p(theta1 | x1) is retrieved
	actual = self.p_test.probability_of_hypothesis_given_data
	# Then it is the expected value
	expect(isclose(actual, 0.014, abs_tol=1e-3)).to(be_true)
	return

    def test_maximum_a_priori(self):
	# Given an instance of the probability
	# When the MAP is retrieved
	actual = self.p_test.maximum_a_priori
	# Then it is the expected label
	expect(actual).to(equal("theta4"))
	return

    def __call__(self):
	tests = (thing for thing in dir(self) if thing.startswith("test_"))
	for test in tests:
	    getattr(self, test)()
	return
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test = TestProbability(joint)
test()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org45dd9eb" class="outline-3"&gt;
&lt;h3 id="org45dd9eb"&gt;Source&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org45dd9eb"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Stone JV. Bayes’ rule: a tutorial introduction to Bayesian analysis. First edition, third printing [with corrections]. Sheffield: Sebtel Press; 2014. 170 p.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>bayes</category><category>probability</category><guid>https://necromuralist.github.io/A-Likely-Story/posts/bayes/joint-probability-functions/</guid><pubDate>Wed, 12 Jun 2019 19:33:59 GMT</pubDate></item></channel></rss>